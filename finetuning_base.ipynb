{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_utils import *\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = SSTDataset(filename = 'train.tsv', maxlen = 512)\n",
    "val_set = SSTDataset(filename = 'val.tsv', maxlen = 512)\n",
    "test_set = SSTDataset(filename ='test.tsv', maxlen = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, freeze_bert = True):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        #Instantiating BERT model object \n",
    "        self.bert_layer = BertModel.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "        #Freeze bert layers\n",
    "        if freeze_bert:\n",
    "            for p in self.bert_layer.parameters():\n",
    "                p.requires_grad = False\n",
    "        \n",
    "        #Classification layer\n",
    "        self.cls_layer = nn.Linear(768, 1)\n",
    "\n",
    "    def forward(self, seq, attn_masks):\n",
    "        '''\n",
    "        Inputs:\n",
    "            -seq : Tensor of shape [B, T] containing token ids of sequences\n",
    "            -attn_masks : Tensor of shape [B, T] containing attention masks to be used to avoid contibution of PAD tokens\n",
    "        '''\n",
    "\n",
    "        #Feeding the input to BERT model to obtain contextualized representations\n",
    "        cont_reps, _ = self.bert_layer(seq, attention_mask = attn_masks)\n",
    "\n",
    "        #Obtaining the representation of [CLS] head\n",
    "        cls_rep = cont_reps[:, 0]\n",
    "\n",
    "        #Feeding cls_rep to the classifier layer\n",
    "        logits = self.cls_layer(cls_rep)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = SentimentClassifier(freeze_bert = True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, child in enumerate(net.bert_layer.encoder.layer.children()):\n",
    "    if i<10:\n",
    "        for p in child.parameters():\n",
    "            p.requires_grad = False\n",
    "    else:\n",
    "        for p in child.parameters():\n",
    "            p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "opti = optim.Adam(net.parameters(), lr = 2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_from_logits(logits, labels):\n",
    "    probs = torch.sigmoid(logits.unsqueeze(-1))\n",
    "    soft_probs = (probs > 0.5).long()\n",
    "    acc = (soft_probs.squeeze() == labels).float().mean()\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, criterion, optimizer, dataset, batch_size=32):\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "    model.train()\n",
    "    train_loss, train_acc, count = 0.0, 0.0, 0\n",
    "    for seq, attn_masks, labels in tqdm(dataloader):\n",
    "        seq, attn_masks, labels = seq.to(device), attn_masks.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logits = model(seq, attn_masks)  \n",
    "        loss = criterion(logits.squeeze(-1), labels.float())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        count += 1 \n",
    "    train_loss /= count\n",
    "    return model, train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_one_epoch(model, criterion, optimizer, dataset, batch_size=32):\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=batch_size, num_workers = 5\n",
    "    )\n",
    "    model.eval()\n",
    "    loss, acc, count = 0.0, 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for seq, attn_masks, labels in tqdm(dataloader):\n",
    "            seq, attn_masks, labels = seq.to(device), attn_masks.to(device), labels.to(device)\n",
    "            logits = model(seq, attn_masks)\n",
    "            loss += criterion(logits.squeeze(-1), labels.float()).item()\n",
    "            acc += get_accuracy_from_logits(logits, labels)\n",
    "            count += 1\n",
    "        loss /= count\n",
    "        acc /= count\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "test_losses = []\n",
    "val_accs = []\n",
    "test_accs = []\n",
    "def train(net, criterion, opti, trainset, valset, testset, batch_size=32):\n",
    "    for ep in range(5):\n",
    "        net, train_loss = train_one_epoch(\n",
    "                net, criterion, opti, trainset, batch_size=batch_size)\n",
    "        val_loass, val_acc  = evaluate_one_epoch(\n",
    "            net, criterion, opti, valset, batch_size=batch_size)\n",
    "        #test_loss, test_acc = evaluate_one_epoch(\n",
    "        #    net, criterion, opti, testset, batch_size=batch_size)\n",
    "        train_losses.append(train_loss)\n",
    "        #test_losses.append(test_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        #test_accs.append(test_acc)\n",
    "        val_accs.append(val_acc)\n",
    "        return net, train_losses, val_losses, val_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net, train_losses, val_losses, val_accs = train(net,\\\n",
    "                                             criterion,\\\n",
    "                                             opti,\\\n",
    "                                             train_set,\n",
    "                                             val_set,\n",
    "                                             test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
